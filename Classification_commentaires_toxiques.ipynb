{"cells":[{"cell_type":"markdown","metadata":{"id":"_lyJzHuKRNWX"},"source":["# Importation des packages"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MUj96QiIQsJ_"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to C:\\Users\\Samy\n","[nltk_data]     Bouhelassa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to C:\\Users\\Samy\n","[nltk_data]     Bouhelassa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samy\n","[nltk_data]     Bouhelassa\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","#import tensorflow_addons as tfa\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPool1D, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"xZLpUMItRWc0"},"source":["# Importation des données"]},{"cell_type":"markdown","metadata":{"id":"cbE3MzVKRaV2"},"source":["Ajoutez un raccourci de ce dossier à votre google drive :\n","\n","https://drive.google.com/drive/folders/1mx-CAzT10YKrmxHfYDP_1Oef7PVGUr7s?usp=sharing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":2823,"status":"ok","timestamp":1645598103254,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"},"user_tz":-60},"id":"qky64Lq8Rocy","outputId":"3f57ca8f-59ae-4e38-e35d-a040b2e6a944"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('./data/train.csv')\n","data.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Lecture des données\n","train = pd.read_csv('./data/train.csv')\n","test = pd.read_csv('./data/test.csv')\n","\n","# On ne garde que les 5000 premiers exemples (pour les tests)\n","train = train[:15000]\n","\n","# On supprime la colonne 'id' qui ne nous sert pas\n","train = train.drop('id', axis=1)\n","\n","# Nombre de commentaires possédant un nombre de catégories\n","labels = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n","labels = np.sum(labels, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"-kUWzBzISpsK"},"source":["# Etude du jeu de données"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CAexGL7jS-VF"},"outputs":[{"data":{"text/plain":["toxic            1439\n","severe_toxic      158\n","obscene           789\n","threat             50\n","insult            749\n","identity_hate     133\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Your Code\n","\n","# On affiche le nombre de commentaires de chaque catégorie\n","train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Commentaire pour la colonne toxic :\n","\"85252    \"\\n Ok, first of all, the reason I didn't dele...\n","Name: comment_text, dtype: object\"\n","\n","Commentaire pour la colonne severe_toxic :\n","\"157382    Sorry bitch but the miz is a dumb faggot ass p...\n","Name: comment_text, dtype: object\"\n","\n","Commentaire pour la colonne obscene :\n","\"121230    \"\\n\\n You fucking no-life car fetishist \\n\\nSi...\n","Name: comment_text, dtype: object\"\n","\n","Commentaire pour la colonne threat :\n","\"129384    Unban this ip address or a new online encyclop...\n","Name: comment_text, dtype: object\"\n","\n","Commentaire pour la colonne insult :\n","\"41679    o feel sorry for you, fuck off you american ba...\n","Name: comment_text, dtype: object\"\n","\n","Commentaire pour la colonne identity_hate :\n","\"124389    \"\\n\\nhaha you are a dumb shit. what's so wrong...\n","Name: comment_text, dtype: object\"\n","\n"]}],"source":["# Affichage d'un commentaire aléatoire pour chaque catégorie\n","\n","for column in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n","    sample = data[data[column] == 1]['comment_text'].sample(1)\n","    print(f\"Commentaire pour la colonne {column} :\\n\\\"{sample}\\\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"YIzpSrAjSsAM"},"source":["# Préparation des données"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>explanation\\nwhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d'aww! he matches this background colour i'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hey man, i'm really not trying to edit war. it...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>you, sir, are my hero. any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        comment_text  toxic  severe_toxic  \\\n","0  explanation\\nwhy the edits made under my usern...      0             0   \n","1  d'aww! he matches this background colour i'm s...      0             0   \n","2  hey man, i'm really not trying to edit war. it...      0             0   \n","3  \"\\nmore\\ni can't make any real suggestions on ...      0             0   \n","4  you, sir, are my hero. any chance you remember...      0             0   \n","\n","   obscene  threat  insult  identity_hate  \n","0        0       0       0              0  \n","1        0       0       0              0  \n","2        0       0       0              0  \n","3        0       0       0              0  \n","4        0       0       0              0  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# On met les commentaires en minuscules\n","train['comment_text'] = train['comment_text'].str.lower()\n","\n","# Affichage des données\n","train.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Explanation, Why, the, edits, made, under, my...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[D, aww, He, matches, this, background, colour...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Hey, man, I, m, really, not, trying, to, edit...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[More, I, can, t, make, any, real, suggestions...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[You, sir, are, my, hero, Any, chance, you, re...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        comment_text  toxic  severe_toxic  \\\n","0  [Explanation, Why, the, edits, made, under, my...      0             0   \n","1  [D, aww, He, matches, this, background, colour...      0             0   \n","2  [Hey, man, I, m, really, not, trying, to, edit...      0             0   \n","3  [More, I, can, t, make, any, real, suggestions...      0             0   \n","4  [You, sir, are, my, hero, Any, chance, you, re...      0             0   \n","\n","   obscene  threat  insult  identity_hate  \n","0        0       0       0              0  \n","1        0       0       0              0  \n","2        0       0       0              0  \n","3        0       0       0              0  \n","4        0       0       0              0  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# On supprime les caractères spéciaux et on sépare les mots avec nltk\n","tokenizer = nltk.RegexpTokenizer(r'\\w+')\n","train['comment_text'] = data['comment_text'].apply(tokenizer.tokenize)\n","\n","train.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Explanation, edits, made, username, Hardcore,...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[aww, matches, background, colour, seemingly, ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Hey, man, really, trying, edit, war, guy, con...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[make, real, suggestions, improvement, wondere...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[sir, hero, chance, remember, page]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        comment_text  toxic  severe_toxic  \\\n","0  [Explanation, edits, made, username, Hardcore,...      0             0   \n","1  [aww, matches, background, colour, seemingly, ...      0             0   \n","2  [Hey, man, really, trying, edit, war, guy, con...      0             0   \n","3  [make, real, suggestions, improvement, wondere...      0             0   \n","4                [sir, hero, chance, remember, page]      0             0   \n","\n","   obscene  threat  insult  identity_hate  \n","0        0       0       0              0  \n","1        0       0       0              0  \n","2        0       0       0              0  \n","3        0       0       0              0  \n","4        0       0       0              0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["stopwords_list = set(stopwords.words('english'))\n","\n","# Fonction pour supprimer les stopwords\n","def remove_stopwords(word_list):\n","    filtered_words = [word for word in word_list if word.lower() not in stopwords_list]\n","    return filtered_words\n","\n","# Appliquer la fonction sur la colonne 'comment_text'\n","train['comment_text'] = train['comment_text'].apply(remove_stopwords)\n","\n","train.head()\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[explanation, edits, made, username, hardcore,...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[aww, match, background, colour, seemingly, st...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[make, real, suggestion, improvement, wondered...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[sir, hero, chance, remember, page]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        comment_text  toxic  severe_toxic  \\\n","0  [explanation, edits, made, username, hardcore,...      0             0   \n","1  [aww, match, background, colour, seemingly, st...      0             0   \n","2  [hey, man, really, trying, edit, war, guy, con...      0             0   \n","3  [make, real, suggestion, improvement, wondered...      0             0   \n","4                [sir, hero, chance, remember, page]      0             0   \n","\n","   obscene  threat  insult  identity_hate  \n","0        0       0       0              0  \n","1        0       0       0              0  \n","2        0       0       0              0  \n","3        0       0       0              0  \n","4        0       0       0              0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Initialiser le lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Fonction pour lemmatiser les mots\n","def lemmatize_words(word_list):\n","    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in word_list]\n","    return lemmatized_words\n","\n","# Appliquer la fonction sur la colonne 'comment_text'\n","train['comment_text'] = train['comment_text'].apply(lemmatize_words)\n","\n","# Afficher les premières lignes du DataFrame après la lemmatisation\n","train.head()\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["['explanation edits made username hardcore metallica fan reverted vandalism closure gas voted new york doll fac please remove template talk page since retired 89 205 38 27',\n"," 'aww match background colour seemingly stuck thanks talk 21 51 january 11 2016 utc',\n"," 'hey man really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Enfin, on reconstitue les phrases\n","sentences = list(map(lambda x : \" \".join(x), train['comment_text']))\n","\n","# Affichage des données\n","sentences[:3]"]},{"cell_type":"markdown","metadata":{"id":"LxNIQgESTCmE"},"source":["# Entraînement du modèle baseline"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"YgkPpzIzSQUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train sequences shape : (13500, 42589)\n","Test sequences shape : (1500, 42589)\n","Train labels shape : (13500,)\n","Test labels shape : (1500,)\n"]}],"source":["# Pour le premier modèle, on va utiliser un Random Forest Classifier\n","# Avant cela, on va vectoriser les phrases avec TF-IDF\n","vectorizer = TfidfVectorizer()\n","\n","# On fit le vectorizer sur les phrases\n","vectorizer.fit(sentences)\n","\n","# On transforme les phrases en vecteurs\n","vectors = vectorizer.transform(sentences)\n","\n","# Séparation des données en train et test\n","train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels, test_size=0.1, random_state=0)\n","\n","# Affichage des dimensions des données\n","print(f\"Train sequences shape : {train_vectors.shape}\")\n","print(f\"Test sequences shape : {test_vectors.shape}\")\n","print(f\"Train labels shape : {train_labels.shape}\")\n","print(f\"Test labels shape : {test_labels.shape}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy : 0.9133333333333333\n"]}],"source":["# Maintenant, on peut créer le modèle\n","model = RandomForestClassifier(n_estimators=15,random_state=0)\n","\n","# Entrainement du modèle\n","model.fit(train_vectors, train_labels)\n","\n","# Prédiction sur les données de test\n","predictions = model.predict(test_vectors)\n","\n","# Affichage de l'accuracy\n","print(f\"Accuracy : {accuracy_score(test_labels, predictions)}\")"]},{"cell_type":"markdown","metadata":{"id":"2a8IWbTFTHXh"},"source":["# Itération de la modélisation "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"rVR0eCkoTQSI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train sequences shape : (13500, 200)\n","Test sequences shape : (1500, 200)\n","Train labels shape : (13500, 6)\n","Test labels shape : (1500, 6)\n"]}],"source":["# Pour le deuxième modèle, on va utiliser un réseau de neurones récurrent (RNN)\n","# Avant cela, on va tokeniser les phrases et leur donner la même longueur\n","\n","# Création du tokenizer\n","tokenizer = Tokenizer(num_words=20000)\n","tokenizer.fit_on_texts(sentences)\n","\n","# Récupération du vocabulaire\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index)\n","\n","# Tokenisation des phrases\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","# Padding des phrases\n","max_length = 200\n","padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n","labels = np.array(train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values)\n","\n","# Séparation des données en train et test\n","train_sequences, test_sequences, train_labels, test_labels = train_test_split(padded, labels, test_size=0.1, random_state=0)\n","\n","# Affichage des dimensions des données\n","print(f\"Train sequences shape : {train_sequences.shape}\")\n","print(f\"Test sequences shape : {test_sequences.shape}\")\n","print(f\"Train labels shape : {train_labels.shape}\")\n","print(f\"Test labels shape : {test_labels.shape}\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class F1Score(tf.keras.metrics.Metric):\n","    def __init__(self, threshold=0.5, num_classes=None, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.threshold = threshold\n","        self.num_classes = num_classes\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred = tf.math.greater_equal(y_pred, self.threshold)\n","        y_true = tf.cast(y_true, tf.bool)\n","\n","        true_positives = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n","        false_positives = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))\n","        false_negatives = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))\n","\n","        self.true_positives.assign_add(tf.reduce_sum(tf.cast(true_positives, tf.float32)))\n","        self.false_positives.assign_add(tf.reduce_sum(tf.cast(false_positives, tf.float32)))\n","        self.false_negatives.assign_add(tf.reduce_sum(tf.cast(false_negatives, tf.float32)))\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n","        f1 = 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n","        return f1\n","\n","    def reset_states(self):\n","        self.true_positives.assign(0)\n","        self.false_positives.assign(0)\n","        self.false_negatives.assign(0)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 200, 100)          4277000   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 200, 60)           38640     \n","                                                                 \n"," global_max_pooling1d_1 (Glo  (None, 60)               0         \n"," balMaxPooling1D)                                                \n","                                                                 \n"," dropout_2 (Dropout)         (None, 60)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 50)                3050      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 6)                 306       \n","                                                                 \n","=================================================================\n","Total params: 4,318,996\n","Trainable params: 4,318,996\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["embedding_dim = 100\n","\n","# Création du modèle\n","model = Sequential([\n","    Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n","    LSTM(units=60, return_sequences=True),\n","    GlobalMaxPool1D(),\n","    Dropout(0.1),\n","    Dense(50, activation='relu'),\n","    Dropout(0.1),\n","    Dense(6, activation='sigmoid')\n","])\n","\n","# Création de la métrique F1\n","f1_score = F1Score(num_classes=6, threshold=0.9)\n","\n","# Compilation du modèle\n","model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n","\n","# Affichage du résumé du modèle\n","model.summary()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","422/422 [==============================] - ETA: 0s - loss: 1.0806 - accuracy: 0.8004 - f1_score: 0.0405"]},{"name":"stderr","output_type":"stream","text":["c:\\anaconda3\\envs\\tenserflow\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n","  m.reset_state()\n"]},{"name":"stdout","output_type":"stream","text":["422/422 [==============================] - 115s 247ms/step - loss: 1.0806 - accuracy: 0.8004 - f1_score: 0.0405 - val_loss: 0.5250 - val_accuracy: 0.9907 - val_f1_score: 0.2716\n","Epoch 2/5\n","422/422 [==============================] - 100s 237ms/step - loss: 2.5306 - accuracy: 0.9913 - f1_score: 0.4234 - val_loss: 1.0889 - val_accuracy: 0.9973 - val_f1_score: 0.4065\n","Epoch 3/5\n","422/422 [==============================] - 100s 238ms/step - loss: 4.4283 - accuracy: 0.9939 - f1_score: 0.4884 - val_loss: 2.0157 - val_accuracy: 0.9973 - val_f1_score: 0.4692\n","Epoch 4/5\n","422/422 [==============================] - 101s 240ms/step - loss: 5.7521 - accuracy: 0.9936 - f1_score: 0.5127 - val_loss: 3.3752 - val_accuracy: 0.9973 - val_f1_score: 0.4297\n","Epoch 5/5\n","422/422 [==============================] - 101s 238ms/step - loss: 9.0562 - accuracy: 0.9939 - f1_score: 0.5422 - val_loss: 6.2353 - val_accuracy: 0.9973 - val_f1_score: 0.4667\n"]}],"source":["num_epochs = 5\n","# num_epochs = 3\n","\n","# Entrainement du modèle\n","history = model.fit(train_sequences, train_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCSqKm9bskmrKsd3y9MwY2","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
